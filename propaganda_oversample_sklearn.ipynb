{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "propaganda_oversample_sklearn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kartikaggarwal98/Propagada-Detection/blob/master/propaganda_oversample_sklearn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCChLzG02teK",
        "colab_type": "code",
        "outputId": "b3b19d1f-86e3-4a7d-de7e-8c5f2648001f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0MNgt1B2tzV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veE50DcO2xrL",
        "colab_type": "code",
        "outputId": "fe70ca2d-9950-49f9-9aa8-810e73063bb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import xgboost as xgb\n",
        "from tqdm import tqdm\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV,train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline,linear_model\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras import layers, models, optimizers\n",
        "import keras\n",
        "import re\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import roc_auc_score\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN9E0ROe21K0",
        "colab_type": "code",
        "outputId": "e791775f-65cf-4ce4-b266-43554419f6b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "stop_words = stopwords.words('english')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUfNmgt_27VA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PATH=Path(\"/content/drive/My Drive/emnlp/\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZUdcxKf3Sf-",
        "colab_type": "code",
        "outputId": "84282d45-7cba-43b8-dc42-2dd22eb2cc83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "os.listdir(PATH)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['datasets', 'Data_Prepare.ipynb', 'traindf.csv']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQsaUNzW3Bpw",
        "colab_type": "code",
        "outputId": "c673b0b5-ea29-4c8c-f959-b38ebe224d85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "df=pd.read_csv(PATH/'traindf.csv')\n",
        "\n",
        "df=df.dropna() ## every alternate line is blank line\n",
        "\n",
        "label_mapping={'propaganda':0,'non-propaganda':1} ## label encoding\n",
        "df['labels']=df['labels'].apply(lambda x:label_mapping[x])\n",
        "df=pd.concat([df,df[df['labels']==0].copy()],0).reset_index(drop=True)\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>sentence</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>US bloggers banned from entering UK</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Two prominent US bloggers have been banned fro...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>Pamela Geller and Robert Spencer co-founded an...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>They were due to speak at an English Defence L...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8</td>\n",
              "      <td>A government spokesman said individuals whose ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0                                           sentence  labels\n",
              "0           0                US bloggers banned from entering UK       1\n",
              "1           2  Two prominent US bloggers have been banned fro...       1\n",
              "2           4  Pamela Geller and Robert Spencer co-founded an...       0\n",
              "3           6  They were due to speak at an English Defence L...       1\n",
              "4           8  A government spokesman said individuals whose ...       1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7J9k3Ay3RGV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ds,labels=df['sentence'],df['labels']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kf7ObHgK7cEl",
        "colab_type": "code",
        "outputId": "9b858218-7aeb-47b4-8577-febb8c77a440",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.unique(labels,return_counts=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 1]), array([ 9440, 11577]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NKf2uiF3uaj",
        "colab_type": "code",
        "outputId": "e23ae0bc-b4f0-4182-aa01-df17f34ff464",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train,X_test,y_train,y_test=train_test_split(ds,labels,test_size=0.2,random_state=42,)\n",
        "X_train.shape,X_test.shape,y_train.shape,y_test.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((16813,), (4204,), (16813,), (4204,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lze1ttzi3luu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a count vectorizer object \n",
        "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
        "# word level tf-idf\n",
        "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
        "# ngram level tf-idf \n",
        "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
        "# characters level tf-idf\n",
        "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EARZBuQn4R7N",
        "colab_type": "code",
        "outputId": "10cb12af-9a2c-4560-d1e3-e02e26621189",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        }
      },
      "source": [
        "print('===========================   Naive Bayes =====================')\n",
        "\n",
        "\n",
        "vectorizers=[(count_vect,'count_vectorizer'),(tfidf_vect,'tfidf_vectorizer_word'),(tfidf_vect_ngram,'tfidf_vectorizer_word_ngram'),(tfidf_vect_ngram_chars,'tfidf_vectorizer_ngram_chars')]\n",
        "\n",
        "#Naive Bayes for all features:\n",
        "for vectorizer in vectorizers:\n",
        "  print(vectorizer[1])\n",
        "  clf = Pipeline([\n",
        "    ('vect',vectorizer[0]),\n",
        "    ('clf', MultinomialNB()),\n",
        "  ])\n",
        "  clf = clf.fit(X_train, y_train)\n",
        "  predictions=clf.predict_proba(X_test)\n",
        "  test_preds=np.argmax(predictions,axis=1)\n",
        "\n",
        "  auc = classification_report(y_test, test_preds)\n",
        "  print (auc)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===========================   Naive Bayes =====================\n",
            "count_vectorizer\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.79      0.72      1851\n",
            "           1       0.80      0.69      0.75      2353\n",
            "\n",
            "    accuracy                           0.73      4204\n",
            "   macro avg       0.74      0.74      0.73      4204\n",
            "weighted avg       0.74      0.73      0.74      4204\n",
            "\n",
            "tfidf_vectorizer_word\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.66      0.67      1851\n",
            "           1       0.74      0.74      0.74      2353\n",
            "\n",
            "    accuracy                           0.71      4204\n",
            "   macro avg       0.70      0.70      0.70      4204\n",
            "weighted avg       0.71      0.71      0.71      4204\n",
            "\n",
            "tfidf_vectorizer_word_ngram\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.64      0.63      1851\n",
            "           1       0.71      0.69      0.70      2353\n",
            "\n",
            "    accuracy                           0.67      4204\n",
            "   macro avg       0.67      0.67      0.67      4204\n",
            "weighted avg       0.67      0.67      0.67      4204\n",
            "\n",
            "tfidf_vectorizer_ngram_chars\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.61      0.63      1851\n",
            "           1       0.71      0.73      0.72      2353\n",
            "\n",
            "    accuracy                           0.68      4204\n",
            "   macro avg       0.67      0.67      0.67      4204\n",
            "weighted avg       0.68      0.68      0.68      4204\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8NbsLS74wGo",
        "colab_type": "code",
        "outputId": "f13e53a7-5cb8-476b-d161-84b26b693021",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 759
        }
      },
      "source": [
        "print('===========================   Logistic Regression =====================')\n",
        "\n",
        "\n",
        "vectorizers=[(count_vect,'count_vectorizer'),(tfidf_vect,'tfidf_vectorizer_word'),(tfidf_vect_ngram,'tfidf_vectorizer_word_ngram'),(tfidf_vect_ngram_chars,'tfidf_vectorizer_ngram_chars')]\n",
        "\n",
        "for vectorizer in vectorizers:\n",
        "  print(vectorizer[1])\n",
        "  clf = Pipeline([\n",
        "    ('vect',vectorizer[0]),\n",
        "    ('clf', linear_model.LogisticRegression(multi_class='auto',solver='lbfgs')),\n",
        "  ])\n",
        "  clf = clf.fit(X_train, y_train)\n",
        "  predictions=clf.predict_proba(X_test)\n",
        "  test_preds=np.argmax(predictions,axis=1)\n",
        "\n",
        "  auc = classification_report(y_test, test_preds)\n",
        "  print (auc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===========================   Logistic Regression =====================\n",
            "count_vectorizer\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.76      0.75      1851\n",
            "           1       0.81      0.80      0.80      2353\n",
            "\n",
            "    accuracy                           0.78      4204\n",
            "   macro avg       0.78      0.78      0.78      4204\n",
            "weighted avg       0.78      0.78      0.78      4204\n",
            "\n",
            "tfidf_vectorizer_word\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.66      0.67      1851\n",
            "           1       0.74      0.77      0.75      2353\n",
            "\n",
            "    accuracy                           0.72      4204\n",
            "   macro avg       0.72      0.71      0.71      4204\n",
            "weighted avg       0.72      0.72      0.72      4204\n",
            "\n",
            "tfidf_vectorizer_word_ngram\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.59      0.62      1851\n",
            "           1       0.70      0.75      0.72      2353\n",
            "\n",
            "    accuracy                           0.68      4204\n",
            "   macro avg       0.68      0.67      0.67      4204\n",
            "weighted avg       0.68      0.68      0.68      4204\n",
            "\n",
            "tfidf_vectorizer_ngram_chars\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.65      0.67      1851\n",
            "           1       0.74      0.76      0.75      2353\n",
            "\n",
            "    accuracy                           0.71      4204\n",
            "   macro avg       0.71      0.71      0.71      4204\n",
            "weighted avg       0.71      0.71      0.71      4204\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNG7WedS8J5h",
        "colab_type": "code",
        "outputId": "282b4ff8-139f-4722-fb06-b9fedd908a0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        }
      },
      "source": [
        "print('===========================   SVM =====================')\n",
        "\n",
        "\n",
        "vectorizers=[(count_vect,'count_vectorizer'),(tfidf_vect,'tfidf_vectorizer_word'),(tfidf_vect_ngram,'tfidf_vectorizer_word_ngram'),(tfidf_vect_ngram_chars,'tfidf_vectorizer_ngram_chars')]\n",
        "\n",
        "# SVM on count vectors: SVM Classifier Pipeline on word count vector\n",
        "for vectorizer in vectorizers:\n",
        "  print(vectorizer[1])\n",
        "  clf = Pipeline([\n",
        "    ('vect',vectorizer[0]),\n",
        "    ('clf', SVC(gamma='scale',probability=True)),\n",
        "  ])\n",
        "  clf = clf.fit(X_train, y_train)\n",
        "  predictions=clf.predict_proba(X_test)\n",
        "  test_preds=np.argmax(predictions,axis=1)\n",
        "\n",
        "  auc = classification_report(y_test, test_preds)\n",
        "  print (auc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===========================   SVM =====================\n",
            "count_vectorizer\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.73      0.75      1851\n",
            "           1       0.79      0.82      0.81      2353\n",
            "\n",
            "    accuracy                           0.78      4204\n",
            "   macro avg       0.78      0.78      0.78      4204\n",
            "weighted avg       0.78      0.78      0.78      4204\n",
            "\n",
            "tfidf_vectorizer_word\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.80      0.80      1851\n",
            "           1       0.84      0.84      0.84      2353\n",
            "\n",
            "    accuracy                           0.82      4204\n",
            "   macro avg       0.82      0.82      0.82      4204\n",
            "weighted avg       0.82      0.82      0.82      4204\n",
            "\n",
            "tfidf_vectorizer_word_ngram\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.75      0.75      1851\n",
            "           1       0.81      0.81      0.81      2353\n",
            "\n",
            "    accuracy                           0.78      4204\n",
            "   macro avg       0.78      0.78      0.78      4204\n",
            "weighted avg       0.78      0.78      0.78      4204\n",
            "\n",
            "tfidf_vectorizer_ngram_chars\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.79      0.79      1851\n",
            "           1       0.83      0.84      0.84      2353\n",
            "\n",
            "    accuracy                           0.82      4204\n",
            "   macro avg       0.82      0.81      0.81      4204\n",
            "weighted avg       0.82      0.82      0.82      4204\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qlw427qb_8Wq",
        "colab_type": "text"
      },
      "source": [
        "Dev Set Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQn1-9kMaRVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_folder = \"/content/drive/My Drive/emnlp/datasets/train-articles\" # check that the path to the datasets folder is correct,\n",
        "dev_folder = \"/content/drive/My Drive/emnlp/datasets/dev-articles\"     # if not adjust these variables accordingly\n",
        "train_labels_folder = \"/content/drive/My Drive/emnlp/datasets/train-labels-SLC\"\n",
        "dev_template_labels_file = \"/content/drive/My Drive/emnlp/datasets/dev.template-output-SLC.out\"\n",
        "task_SLC_output_file = \"/content/drive/My Drive/emnlp/baseline-output-SLC1.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFMjb8vDXygo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "import os.path\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "\n",
        "def read_articles_from_file_list(folder_name, file_pattern=\"*.txt\"):\n",
        "    \n",
        "    file_list = glob.glob(os.path.join(folder_name, file_pattern))\n",
        "    article_id_list, sentence_id_list, sentence_list = ([], [], [])\n",
        "    for filename in sorted(file_list):\n",
        "        article_id = os.path.basename(filename).split(\".\")[0][7:]\n",
        "        with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "            for sentence_id, row in enumerate(f.readlines(), 1):\n",
        "                sentence_list.append(row.rstrip())\n",
        "                article_id_list.append(article_id)\n",
        "                sentence_id_list.append(str(sentence_id))\n",
        "\n",
        "    return article_id_list, sentence_id_list, sentence_list\n",
        "\n",
        "\n",
        "def are_ids_aligned(article_id_list, sentence_id_list,\n",
        "                    reference_article_id_list, reference_sentence_id_list):\n",
        "    \"\"\"\n",
        "    check whether the two lists of ids of the articles and the sentences are aligned\n",
        "    \"\"\"\n",
        "    for art, ref_art, sent, ref_sent in zip(article_id_list, reference_article_id_list,\n",
        "                                            sentence_id_list, reference_sentence_id_list):\n",
        "        if art != ref_art:\n",
        "            print(\"ERROR: article ids do not match: article id = %s, reference article id = %s\"%(art, ref_art))\n",
        "            return False\n",
        "        if sent != ref_sent:\n",
        "            print(\"ERROR: sentence ids do not match: article id:%s,%s sentence id:%s,%s\" %(art, ref_art, sent, ref_sent))\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def read_predictions_from_file(filename):\n",
        "   \n",
        "    articles_id, sentence_id_list, gold_labels = ([], [], [])\n",
        "    with open(filename, \"r\") as f:\n",
        "        for row in f.readlines():\n",
        "            article_id, sentence_id, gold_label = row.rstrip().split(\"\\t\")\n",
        "            articles_id.append(article_id)\n",
        "            sentence_id_list.append(sentence_id)\n",
        "            gold_labels.append(gold_label)\n",
        "    return articles_id, sentence_id_list, gold_labels\n",
        "\n",
        "\n",
        "def read_predictions_from_file_list(folder_name, file_pattern):\n",
        "    gold_file_list = glob.glob(os.path.join(folder_name, file_pattern))\n",
        "    articles_id, sentence_id_list, gold_labels = ([], [], [])\n",
        "    for filename in sorted(gold_file_list):\n",
        "        art_ids, sent_ids, golds = read_predictions_from_file(filename)\n",
        "        articles_id += art_ids\n",
        "        sentence_id_list += sent_ids\n",
        "        gold_labels += golds\n",
        "    return articles_id, sentence_id_list, gold_labels\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4uUZuA5RESK",
        "colab_type": "code",
        "outputId": "e22f4f1f-c7d7-4776-c676-4c4b80c4dc3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "os.listdir('/content/drive/My Drive/emnlp/datasets/')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dev-articles',\n",
              " 'train-labels-FLC',\n",
              " 'dev-template-output-SLC',\n",
              " 'README.md',\n",
              " 'train.task-SLC.labels',\n",
              " 'train-articles',\n",
              " 'train-labels-SLC',\n",
              " 'dev.template-output-SLC.out']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIe-Y68YAOJ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reading data from the development set\n",
        "dev_article_id_list, dev_sentence_id_list, dev_sentence_list = read_articles_from_file_list(dev_folder)\n",
        "reference_articles_id, reference_sentence_id_list, dev_labels = read_predictions_from_file(dev_template_labels_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njiGKBDQaVRo",
        "colab_type": "code",
        "outputId": "b9958287-4416-4790-f657-baaa343f8ad7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(dev_article_id_list),len(reference_articles_id)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2235, 2235)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6EV9mZxlx_M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not are_ids_aligned(dev_article_id_list, dev_sentence_id_list, reference_articles_id, reference_sentence_id_list):\n",
        "    sys.exit(\"Exiting: development set article ids and gold labels are not aligned\")\n",
        "\n",
        "# computing the predictions on the development set\n",
        "dev=dev_sentence_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qcbDF2uRnW4",
        "colab_type": "code",
        "outputId": "1eaa5f68-0b29-4455-c661-20a0e932c8d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "clf = Pipeline([\n",
        "    ('vect',tfidf_vect),\n",
        "    ('clf', SVC(gamma='scale',probability=True)),\n",
        "  ])\n",
        "clf = clf.fit(X_train, y_train)\n",
        "predictions=clf.predict_proba(X_test)\n",
        "test_preds=np.argmax(predictions,axis=1)\n",
        "\n",
        "auc = classification_report(y_test, test_preds)\n",
        "print (auc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.80      0.80      1851\n",
            "           1       0.84      0.84      0.84      2353\n",
            "\n",
            "    accuracy                           0.82      4204\n",
            "   macro avg       0.82      0.82      0.82      4204\n",
            "weighted avg       0.82      0.82      0.82      4204\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avStlJvanwws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = clf.predict(dev)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IKYfRFJTDsJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_inverse_mapping={0:'propaganda',1:'non-propaganda'} ## label encoding\n",
        "\n",
        "predictions=np.array([label_inverse_mapping[x] for x in preds])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXOTQPdCTOcO",
        "colab_type": "code",
        "outputId": "58523993-a022-425d-8c24-04dbfe55aa32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predictions.shape,preds.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2235,), (2235,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMtfdinvSxzM",
        "colab_type": "code",
        "outputId": "a737e7f4-f5dd-4f3d-d88d-c1609c3b9fc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "np.unique(predictions,return_counts=True),np.unique(preds,return_counts=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((array(['non-propaganda', 'propaganda'], dtype='<U14'), array([1581,  654])),\n",
              " (array([0, 1]), array([ 654, 1581])))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdWmJv12DRot",
        "colab_type": "code",
        "outputId": "07f4265c-ff46-497f-d3a7-c310294f3003",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "task_SLC_output_file = \"/content/drive/My Drive/emnlp/submissions/SLC1.txt\"\n",
        "# writing predictions to file\n",
        "with open(task_SLC_output_file, \"w\") as fout:\n",
        "    for article_id, sentence_id, prediction in zip(dev_article_id_list, dev_sentence_id_list, predictions):\n",
        "        fout.write(\"%s\\t%s\\t%s\\n\" % (article_id, sentence_id, prediction))\n",
        "print(\"Predictions written to file \" + task_SLC_output_file)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predictions written to file /content/drive/My Drive/emnlp/submissions/SLC1.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-6N1UFOSWjE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}